{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features_modify=pd.read_csv(\"/Users/sima/Documents/Biasly/features.csv\",encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_modify.iloc[:, 4:14]\n",
    "\n",
    "y = features_modify[['Final Label']].values.tolist()\n",
    "\n",
    "sentences = features_modify.iloc[:, 3].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The privilege exists solely for the protection of the witness himself, and may not be claimed for the benefit of third parties.\n",
      "[1, 424, 2776, 1357, 13, 1, 729, 2, 1, 327, 49, 5, 28, 17, 11, 1057, 13, 1, 730, 2, 544, 731]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 500)               1066000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 1,226,501\n",
      "Trainable params: 1,226,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "1114/1114 [==============================] - 217s 195ms/step - loss: 0.6949 - acc: 0.4811\n",
      "Epoch 2/10\n",
      "1114/1114 [==============================] - 202s 181ms/step - loss: 0.6934 - acc: 0.4883\n",
      "Epoch 3/10\n",
      "1114/1114 [==============================] - 219s 197ms/step - loss: 0.6940 - acc: 0.4991\n",
      "Epoch 4/10\n",
      "1114/1114 [==============================] - 598s 537ms/step - loss: 0.6932 - acc: 0.5081\n",
      "Epoch 5/10\n",
      "1114/1114 [==============================] - 443s 398ms/step - loss: 0.6933 - acc: 0.5108\n",
      "Epoch 6/10\n",
      "1114/1114 [==============================] - 416s 373ms/step - loss: 0.6930 - acc: 0.5108\n",
      "Epoch 7/10\n",
      "1114/1114 [==============================] - 133s 119ms/step - loss: 0.6934 - acc: 0.4973\n",
      "Epoch 8/10\n",
      "1114/1114 [==============================] - 135s 121ms/step - loss: 0.6930 - acc: 0.5108\n",
      "Epoch 9/10\n",
      "1114/1114 [==============================] - 130s 116ms/step - loss: 0.6936 - acc: 0.5081\n",
      "Epoch 10/10\n",
      "1114/1114 [==============================] - 178s 159ms/step - loss: 0.6933 - acc: 0.5117\n",
      "Accuracy: 45.40%\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.30)\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(sentences_train[2])\n",
    "print(X_train[2])\n",
    "\n",
    "top_words = 5000\n",
    "embedding_vecor_length = 100\n",
    "maxlen = 500\n",
    "max_review_length = 500\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(500))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1114/1114 [==============================] - 147s 132ms/step - loss: 3.5598 - acc: 0.5018\n",
      "Epoch 2/10\n",
      "1114/1114 [==============================] - 162s 146ms/step - loss: 1.1573 - acc: 0.4910\n",
      "Epoch 3/10\n",
      "1114/1114 [==============================] - 157s 141ms/step - loss: 0.7423 - acc: 0.4910\n",
      "Epoch 4/10\n",
      "1114/1114 [==============================] - 140s 126ms/step - loss: 0.6965 - acc: 0.4910\n",
      "Epoch 5/10\n",
      "1114/1114 [==============================] - 145s 130ms/step - loss: 0.6934 - acc: 0.4910\n",
      "Epoch 6/10\n",
      "1114/1114 [==============================] - 140s 126ms/step - loss: 0.6932 - acc: 0.4910\n",
      "Epoch 7/10\n",
      "1114/1114 [==============================] - 131s 118ms/step - loss: 0.6931 - acc: 0.5090\n",
      "Epoch 8/10\n",
      "1114/1114 [==============================] - 129s 115ms/step - loss: 0.6931 - acc: 0.5090\n",
      "Epoch 9/10\n",
      "1114/1114 [==============================] - 142s 128ms/step - loss: 0.6931 - acc: 0.5090\n",
      "Epoch 10/10\n",
      "1114/1114 [==============================] - 146s 131ms/step - loss: 0.6931 - acc: 0.5090\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model expects 2 input arrays, but only received one array. Found: array with shape (478, 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-925b0f8e8787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentences_train_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sima/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1724\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sima/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1415\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sima/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    118\u001b[0m                              \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                              \u001b[0;34m' arrays, but only received one array. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                              'Found: array with shape ' + str(data.shape))\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The model expects 2 input arrays, but only received one array. Found: array with shape (478, 500)"
     ]
    }
   ],
   "source": [
    "# LSTM with the features\n",
    "from keras.models import Model\n",
    "import keras\n",
    "# \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import concatenate\n",
    "import numpy as np\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.30, random_state=1)\n",
    "\n",
    "sentences_train_meta, sentences_test_meta, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=500)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "top_words = 500\n",
    "embedding_vecor_length = 500\n",
    "maxlen = 500\n",
    "max_review_length = 500\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "embedding_vecor_length = 500\n",
    "data_meta = np.array((len(X), 10))\n",
    "\n",
    "nlp_input = Input(shape=(500,), name='nlp_input')\n",
    "meta_input = Input(shape=(10,), name='meta_input')\n",
    "emb = Embedding(output_dim=500, input_dim=500, input_length=500)(nlp_input)\n",
    "nlp_out = Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=regularizers.l2(0.01)))(emb)\n",
    "x = concatenate([nlp_out, meta_input])\n",
    "x = Dense(1, activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=[nlp_input , meta_input], outputs=[x])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.fit([X_train,sentences_train_meta], y_train, batch_size=32, epochs=10)\n",
    "# scores = model.evaluate([X_test,, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = (478,10)\n",
    "ss = np.zeros(s)\n",
    "ss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.82%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate([X_test,ss], y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1114/1114 [==============================] - 134s 120ms/step - loss: 3.3603 - acc: 0.5314\n",
      "Epoch 2/10\n",
      "1114/1114 [==============================] - 129s 116ms/step - loss: 1.1025 - acc: 0.6544\n",
      "Epoch 3/10\n",
      "1114/1114 [==============================] - 135s 121ms/step - loss: 0.6565 - acc: 0.7262\n",
      "Epoch 4/10\n",
      "1114/1114 [==============================] - 149s 134ms/step - loss: 0.5766 - acc: 0.7496\n",
      "Epoch 5/10\n",
      "1114/1114 [==============================] - 137s 123ms/step - loss: 0.5330 - acc: 0.7801\n",
      "Epoch 6/10\n",
      "1114/1114 [==============================] - 176s 158ms/step - loss: 0.5093 - acc: 0.8043\n",
      "Epoch 7/10\n",
      "1114/1114 [==============================] - 191s 171ms/step - loss: 0.4921 - acc: 0.8016\n",
      "Epoch 8/10\n",
      "1114/1114 [==============================] - 201s 180ms/step - loss: 0.4556 - acc: 0.8348\n",
      "Epoch 9/10\n",
      "1114/1114 [==============================] - 232s 208ms/step - loss: 0.4387 - acc: 0.8501\n",
      "Epoch 10/10\n",
      "1114/1114 [==============================] - 222s 200ms/step - loss: 0.4324 - acc: 0.8546\n",
      "Accuracy: 60.67%\n"
     ]
    }
   ],
   "source": [
    "# lets use minmaxscalar to transform the extra knwoledge then add it to LSTM\n",
    "from keras.models import Model\n",
    "import keras\n",
    "# \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import concatenate\n",
    "import numpy as np\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# modifying the extra data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_normalized = scaler.transform(X)\n",
    "\n",
    "\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.30, random_state=1)\n",
    "\n",
    "sentences_train_meta, sentences_test_meta, y_train_1, y_test_1 = train_test_split(X_normalized, y, test_size=0.30, random_state=1)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=500)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "top_words = 500\n",
    "embedding_vecor_length = 500\n",
    "maxlen = 500\n",
    "max_review_length = 500\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "embedding_vecor_length = 500\n",
    "data_meta = np.array((len(X), 10))\n",
    "nlp_input = Input(shape=(500,), name='nlp_input')\n",
    "meta_input = Input(shape=(10,), name='meta_input')\n",
    "emb = Embedding(output_dim=500, input_dim=500, input_length=500)(nlp_input)\n",
    "nlp_out = Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=regularizers.l2(0.01)))(emb)\n",
    "x = concatenate([nlp_out, meta_input])\n",
    "x = Dense(1, activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=[nlp_input , meta_input], outputs=[x])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit([X_train,sentences_train_meta], y_train, batch_size=32, epochs=10)\n",
    "scores = model.evaluate([X_test, ss], y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.63 (+/- 0.02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# tfidf and sdg classifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "vectorizer = TfidfVectorizer()\n",
    "XX = vectorizer.fit_transform(sentences)\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.30, random_state=1)\n",
    "clf = linear_model.SGDClassifier(max_iter=20000, tol=1e-1)\n",
    "# clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "# predicted = clf.predict(X_test)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i will preprocess the normalized input\n",
    "npm_tfidf = XX.todense()\n",
    "combined_text_features=[]\n",
    "for i in range(0,1592):\n",
    "    Feat = list(X_normalized[i])\n",
    "    TFIDF = np.array(npm_tfidf[i]).reshape(-1,).tolist()\n",
    "    t = TFIDF + Feat\n",
    "    combined_text_features.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61 (+/- 0.04)\n",
      "Average precision-recall score: 0.59\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# XX = vectorizer.fit_transform(sentences)\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_text_features, y, test_size=0.30, random_state=1)\n",
    "clf = linear_model.SGDClassifier(max_iter=20000, tol=1e-1)\n",
    "clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "average_precision = average_precision_score(y_test, predicted)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.63 (+/- 0.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# lets try naive bayes\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "gnb = MultinomialNB()\n",
    "vectorizer = TfidfVectorizer()\n",
    "XX = vectorizer.fit_transform(sentences)\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.30, random_state=1)\n",
    "# clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(gnb, X_train.toarray(), y_train, cv=5)\n",
    "# predicted = clf.predict(X_test)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58 (+/- 0.08)\n",
      "Average precision-recall score: 0.54\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# XX = vectorizer.fit_transform(sentences)\n",
    "gnb = GaussianNB()\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_text_features, y, test_size=0.30, random_state=1)\n",
    "# clf = linear_model.SGDClassifier(max_iter=20000, tol=1e-1)\n",
    "gnb.fit(X_train, y_train)\n",
    "scores = cross_val_score(gnb, X_train, y_train, cv=5)\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "average_precision = average_precision_score(y_test, predicted)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.51 (+/- 0.00)\n",
      "Average precision-recall score: 0.54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# XX = vectorizer.fit_transform(sentences)\n",
    "clf = SVC(gamma='auto')\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_text_features, y, test_size=0.30, random_state=1)\n",
    "# clf = linear_model.SGDClassifier(max_iter=20000, tol=1e-1)\n",
    "clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "average_precision = average_precision_score(y_test, predicted)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.51 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "clf = SVC(gamma='auto')\n",
    "vectorizer = TfidfVectorizer()\n",
    "XX = vectorizer.fit_transform(sentences)\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.30, random_state=1)\n",
    "# clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(clf, X_train.toarray(), y_train, cv=5)\n",
    "# predicted = clf.predict(X_test)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.51 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "# xgboost\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "LR = LogisticRegression(penalty='l1',C=0.1)\n",
    "vectorizer = TfidfVectorizer()\n",
    "XX = vectorizer.fit_transform(sentences)\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.30, random_state=1)\n",
    "LR.fit(X_train, y_train)\n",
    "scores = cross_val_score(LR, X_train.toarray(), y_train, cv=5)\n",
    "predicted = LR.predict(X_test)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.51 (+/- 0.00)\n",
      "Average precision-recall score: 0.54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# XX = vectorizer.fit_transform(sentences)\n",
    "LR = LogisticRegression(penalty='l1',C=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_text_features, y, test_size=0.30, random_state=1)\n",
    "# clf = linear_model.SGDClassifier(max_iter=20000, tol=1e-1)\n",
    "LR.fit(X_train, y_train)\n",
    "scores = cross_val_score(LR, X_train, y_train, cv=5)\n",
    "predicted = LR.predict(X_test)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "average_precision = average_precision_score(y_test, predicted)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1592, 5)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X_new = SelectKBest(chi2, k=5).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfeatures = SelectKBest(score_func=chi2, k=5)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sima/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09873356  0.1297592   0.00454501  0.11050467  0.07030426  0.12670164\n",
      "  0.10907707  0.17096958  0.13566476  0.04374023]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoMAAAFdCAYAAAB4smdMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzOzGvNk0MzMzKzGPBk0MzMzq7F/AcOzr0DFFdqaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13456bda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Tense</th>\n",
       "      <th>NP Pattern</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Symbol Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6549</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6704</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6566</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6519</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.6800</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6924</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.7037</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.6784</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.6531</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.6657</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.6619</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.6607</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.6662</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.6955</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.3423</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.6668</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.6490</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.6545</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.6863</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>0.6467</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>0.6579</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>0.6961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>0.6678</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>0.6633</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>0.6639</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>0.6268</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>0.6818</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>0.6795</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>0.9224</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>0.9631</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>0.8448</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>0.5722</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>0.8067</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>0.8814</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>0.9620</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>0.8357</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>0.9236</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>0.8845</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>0.8033</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>0.8840</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>0.8438</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1592 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Confidence  Tense  NP Pattern  Word Count  Symbol Count\n",
       "0         0.6549    1.0           1          38             8\n",
       "1         0.6704    1.0           4          17             5\n",
       "2         0.6308    1.0          12          13             5\n",
       "3         0.6566    2.0           2          42             5\n",
       "4         1.0000    3.0           7          26             7\n",
       "5         0.6519    7.0           2          59            34\n",
       "6         1.0000    1.0           1          26             7\n",
       "7         0.6800    3.0           1          42            11\n",
       "8         0.6924    3.0           1          15             4\n",
       "9         0.7037    1.0           4          25             4\n",
       "10        0.6784    4.0           1          35             7\n",
       "11        1.0000    2.0          15          31            13\n",
       "12        0.6531    4.0           1          22             8\n",
       "13        0.6657    1.0           2          72            10\n",
       "14        0.6619    3.0           1          43             7\n",
       "15        0.6607    3.0           2          39             6\n",
       "16        1.0000    3.0           3          23             5\n",
       "17        0.6662    4.0           2          74            12\n",
       "18        1.0000    2.0           3          38             8\n",
       "19        1.0000    2.0           4          72             8\n",
       "20        0.6955    7.0           2          68            11\n",
       "21        1.0000    1.0          16          21             2\n",
       "22        0.3423    2.0           1          31             5\n",
       "23        0.6668    2.0           2          70            11\n",
       "24        0.6490    2.0           1          53             5\n",
       "25        0.6545    2.0           8          48             6\n",
       "26        0.6863    4.0           5          27             4\n",
       "27        1.0000    3.0           2          11             3\n",
       "28        1.0000    2.0           7          44             6\n",
       "29        1.0000    2.0           2          62             5\n",
       "...          ...    ...         ...         ...           ...\n",
       "1562      0.6467    2.0           1          34             4\n",
       "1563      0.6579    1.0           4          74             8\n",
       "1564      0.6961    1.0           6          21             2\n",
       "1565      1.0000    1.0          17          22             2\n",
       "1566      1.0000    2.0           2          25             3\n",
       "1567      0.6678    2.0           2          44             2\n",
       "1568      0.6633    3.0           1          20             4\n",
       "1569      0.6639    2.0           1          38             3\n",
       "1570      0.6268    6.0           2          24             3\n",
       "1571      0.6818    4.0           2          22             3\n",
       "1572      0.6795    3.0           2          35             5\n",
       "1573      1.0000    5.0           2          14             1\n",
       "1574      0.9224    3.0           3           8             1\n",
       "1575      0.9631    3.0           4          12             1\n",
       "1576      1.0000    1.0           3          17             1\n",
       "1577      0.8448    3.0           8          10             2\n",
       "1578      0.5722    5.0           1           8             1\n",
       "1579      0.8067    1.0           2          19             3\n",
       "1580      0.8814    3.0           1          12             2\n",
       "1581      0.9620    2.0           7          16             1\n",
       "1582      0.8357    1.0           5          17             1\n",
       "1583      0.9236    3.0           2          17             3\n",
       "1584      0.8845    1.0           2           9             1\n",
       "1585      0.8033    5.0           1          16             1\n",
       "1586      1.0000    0.0           3          13             1\n",
       "1587      1.0000    1.0           3          23             1\n",
       "1588      1.0000    1.0           2          10             1\n",
       "1589      1.0000    6.0           3          14             2\n",
       "1590      0.8840    5.0           2          44            24\n",
       "1591      0.8438    2.0          13          46             3\n",
       "\n",
       "[1592 rows x 5 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.drop([\"Number of Labelers\", \"Number of Clusters\",\"Modal Type\",\"Corpus\",\"Adverb Exists\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = X.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "npm_tfidf = XX.todense()\n",
    "combined_text_features=[]\n",
    "for i in range(0,1592):\n",
    "    Feat = X_features[i]\n",
    "    TFIDF = np.array(npm_tfidf[i]).reshape(-1,).tolist()\n",
    "    t = TFIDF + Feat\n",
    "    combined_text_features.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sima/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.52 (+/- 0.05)\n",
      "Average precision-recall score: 0.54\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# XX = vectorizer.fit_transform(sentences)\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_text_features, y, test_size=0.30, random_state=1)\n",
    "clf = linear_model.SGDClassifier(max_iter=20000, tol=1e-1)\n",
    "clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "average_precision = average_precision_score(y_test, predicted)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8538"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
